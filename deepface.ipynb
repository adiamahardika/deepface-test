{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./database/family/family.jpg\"\n",
    "img = cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"image\", img)\n",
    "cv2.waitKey(0) \n",
    "  \n",
    "# closing all open windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid detector_backend passed - yolov8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Moonlay\\Adia\\Kerja\\AI\\Python\\Belajar\\deepface\\deepface.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Moonlay/Adia/Kerja/AI/Python/Belajar/deepface/deepface.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m demography \u001b[39m=\u001b[39m DeepFace\u001b[39m.\u001b[39;49mrepresent(img_path \u001b[39m=\u001b[39;49m img, detector_backend \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39myolov8\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\AdiaMahardika\\anaconda3\\envs\\AudienceAnalytics\\lib\\site-packages\\deepface\\DeepFace.py:645\u001b[0m, in \u001b[0;36mrepresent\u001b[1;34m(img_path, model_name, enforce_detection, detector_backend, align, normalization)\u001b[0m\n\u001b[0;32m    643\u001b[0m target_size \u001b[39m=\u001b[39m functions\u001b[39m.\u001b[39mfind_target_size(model_name\u001b[39m=\u001b[39mmodel_name)\n\u001b[0;32m    644\u001b[0m \u001b[39mif\u001b[39;00m detector_backend \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mskip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 645\u001b[0m     img_objs \u001b[39m=\u001b[39m functions\u001b[39m.\u001b[39;49mextract_faces(\n\u001b[0;32m    646\u001b[0m         img\u001b[39m=\u001b[39;49mimg_path,\n\u001b[0;32m    647\u001b[0m         target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[0;32m    648\u001b[0m         detector_backend\u001b[39m=\u001b[39;49mdetector_backend,\n\u001b[0;32m    649\u001b[0m         grayscale\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    650\u001b[0m         enforce_detection\u001b[39m=\u001b[39;49menforce_detection,\n\u001b[0;32m    651\u001b[0m         align\u001b[39m=\u001b[39;49malign,\n\u001b[0;32m    652\u001b[0m     )\n\u001b[0;32m    653\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# skip\u001b[39;00m\n\u001b[0;32m    654\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img_path, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\AdiaMahardika\\anaconda3\\envs\\AudienceAnalytics\\lib\\site-packages\\deepface\\commons\\functions.py:110\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img, target_size, detector_backend, grayscale, enforce_detection, align)\u001b[0m\n\u001b[0;32m    108\u001b[0m     face_objs \u001b[39m=\u001b[39m [(img, img_region, \u001b[39m0\u001b[39m)]\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     face_detector \u001b[39m=\u001b[39m FaceDetector\u001b[39m.\u001b[39;49mbuild_model(detector_backend)\n\u001b[0;32m    111\u001b[0m     face_objs \u001b[39m=\u001b[39m FaceDetector\u001b[39m.\u001b[39mdetect_faces(face_detector, detector_backend, img, align)\n\u001b[0;32m    113\u001b[0m \u001b[39m# in case of no face found\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AdiaMahardika\\anaconda3\\envs\\AudienceAnalytics\\lib\\site-packages\\deepface\\detectors\\FaceDetector.py:39\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(detector_backend)\u001b[0m\n\u001b[0;32m     37\u001b[0m         face_detector_obj[detector_backend] \u001b[39m=\u001b[39m face_detector\n\u001b[0;32m     38\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39minvalid detector_backend passed - \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m detector_backend)\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m face_detector_obj[detector_backend]\n",
      "\u001b[1;31mValueError\u001b[0m: invalid detector_backend passed - yolov8"
     ]
    }
   ],
   "source": [
    "demography = DeepFace.represent(img_path = img, detector_backend = \"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(demography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'emotion': {'angry': 4.460226534774847e-05,\n",
       "   'disgust': 1.237304394141256e-12,\n",
       "   'fear': 2.342242545749773e-09,\n",
       "   'happy': 99.99982118606567,\n",
       "   'sad': 9.64647695056442e-08,\n",
       "   'surprise': 3.3196187132489285e-06,\n",
       "   'neutral': 0.00013500585964720813},\n",
       "  'dominant_emotion': 'happy',\n",
       "  'region': {'x': 969, 'y': 74, 'w': 222, 'h': 263},\n",
       "  'age': 48,\n",
       "  'gender': {'Woman': 0.02803571696858853, 'Man': 99.97196793556213},\n",
       "  'dominant_gender': 'Man',\n",
       "  'race': {'asian': 0.19564579706639051,\n",
       "   'indian': 0.06722019170410931,\n",
       "   'black': 0.005488691385835409,\n",
       "   'white': 95.01534104347229,\n",
       "   'middle eastern': 2.6950249448418617,\n",
       "   'latino hispanic': 2.0212866365909576},\n",
       "  'dominant_race': 'white'},\n",
       " {'emotion': {'angry': 9.318958785323158e-12,\n",
       "   'disgust': 1.648474022872663e-20,\n",
       "   'fear': 4.081635411104687e-18,\n",
       "   'happy': 99.99756216988338,\n",
       "   'sad': 1.1121798589971438e-09,\n",
       "   'surprise': 2.1759998529900544e-11,\n",
       "   'neutral': 0.0024399534230767927},\n",
       "  'dominant_emotion': 'happy',\n",
       "  'region': {'x': 212, 'y': 489, 'w': 185, 'h': 242},\n",
       "  'age': 23,\n",
       "  'gender': {'Woman': 93.59729290008545, 'Man': 6.402705609798431},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 6.227549359885097e-05,\n",
       "   'indian': 3.192571515154001e-06,\n",
       "   'black': 1.4713171980968095e-08,\n",
       "   'white': 99.95874763026862,\n",
       "   'middle eastern': 0.00864311265752798,\n",
       "   'latino hispanic': 0.03254518572633735},\n",
       "  'dominant_race': 'white'},\n",
       " {'emotion': {'angry': 1.8355436092298902e-07,\n",
       "   'disgust': 6.241590756079063e-14,\n",
       "   'fear': 3.130726641227029e-10,\n",
       "   'happy': 99.97994303942406,\n",
       "   'sad': 4.461017689696282e-08,\n",
       "   'surprise': 3.5608319156150528e-06,\n",
       "   'neutral': 0.020045905487261803},\n",
       "  'dominant_emotion': 'happy',\n",
       "  'region': {'x': 401, 'y': 310, 'w': 194, 'h': 263},\n",
       "  'age': 31,\n",
       "  'gender': {'Woman': 0.013516652688849717, 'Man': 99.98648166656494},\n",
       "  'dominant_gender': 'Man',\n",
       "  'race': {'asian': 0.0009447295052927205,\n",
       "   'indian': 0.015501172645809037,\n",
       "   'black': 0.00013153340456451915,\n",
       "   'white': 90.34265394894433,\n",
       "   'middle eastern': 8.088393290234412,\n",
       "   'latino hispanic': 1.552379411920379},\n",
       "  'dominant_race': 'white'},\n",
       " {'emotion': {'angry': 2.9045850790540273e-08,\n",
       "   'disgust': 4.887092405748653e-13,\n",
       "   'fear': 6.743650311277182e-10,\n",
       "   'happy': 99.95919465775458,\n",
       "   'sad': 1.7093831370072015e-06,\n",
       "   'surprise': 1.5204263643956095e-06,\n",
       "   'neutral': 0.04080806927449669},\n",
       "  'dominant_emotion': 'happy',\n",
       "  'region': {'x': 632, 'y': 328, 'w': 185, 'h': 239},\n",
       "  'age': 32,\n",
       "  'gender': {'Woman': 99.99967813491821, 'Man': 0.00032690679745428497},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 1.325624156743288,\n",
       "   'indian': 1.9737111404538155,\n",
       "   'black': 0.5338140297681093,\n",
       "   'white': 37.164708971977234,\n",
       "   'middle eastern': 8.611231297254562,\n",
       "   'latino hispanic': 50.3909170627594},\n",
       "  'dominant_race': 'latino hispanic'},\n",
       " {'emotion': {'angry': 0.0030092240194790065,\n",
       "   'disgust': 1.3721554274759296e-09,\n",
       "   'fear': 3.329927267259336e-06,\n",
       "   'happy': 99.99363422393799,\n",
       "   'sad': 2.673396615193724e-06,\n",
       "   'surprise': 2.399177245759887e-08,\n",
       "   'neutral': 0.0033546828490216285},\n",
       "  'dominant_emotion': 'happy',\n",
       "  'region': {'x': 207, 'y': 46, 'w': 194, 'h': 226},\n",
       "  'age': 45,\n",
       "  'gender': {'Woman': 99.57073926925659, 'Man': 0.42926263995468616},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 6.927174354132148e-05,\n",
       "   'indian': 2.7565408800001023e-05,\n",
       "   'black': 5.400948133171823e-07,\n",
       "   'white': 99.91366267204285,\n",
       "   'middle eastern': 0.0283756700810045,\n",
       "   'latino hispanic': 0.05786825786344707},\n",
       "  'dominant_race': 'white'},\n",
       " {'emotion': {'angry': 2.7995475729292174e-09,\n",
       "   'disgust': 7.44426922974381e-14,\n",
       "   'fear': 2.0049051485827575e-14,\n",
       "   'happy': 99.9998688697815,\n",
       "   'sad': 2.1987483014851428e-09,\n",
       "   'surprise': 1.9190428597343906e-11,\n",
       "   'neutral': 0.00013215637864050223},\n",
       "  'dominant_emotion': 'happy',\n",
       "  'region': {'x': 941, 'y': 449, 'w': 176, 'h': 254},\n",
       "  'age': 30,\n",
       "  'gender': {'Woman': 99.65877532958984, 'Man': 0.34122844226658344},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 3.098582368465941e-06,\n",
       "   'indian': 2.0897816810361292e-07,\n",
       "   'black': 1.0110313627964551e-09,\n",
       "   'white': 99.99234676361084,\n",
       "   'middle eastern': 0.0022710039047524333,\n",
       "   'latino hispanic': 0.00537397172593046},\n",
       "  'dominant_race': 'white'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 , y0 , w0 , h0  = demography[0][\"region\"].values()\n",
    "x1 , y1 , w1 , h1  = demography[1][\"region\"].values()\n",
    "x2 , y2 , w2 , h2  = demography[2][\"region\"].values()\n",
    "x3 , y3 , w3 , h3  = demography[3][\"region\"].values()\n",
    "x4 , y4 , w4 , h4  = demography[4][\"region\"].values()\n",
    "x5 , y5 , w5 , h5  = demography[5][\"region\"].values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 82, 120, 124],\n",
       "        [ 83, 120, 124],\n",
       "        [ 92, 129, 133],\n",
       "        ...,\n",
       "        [205, 208, 206],\n",
       "        [204, 207, 205],\n",
       "        [205, 208, 206]],\n",
       "\n",
       "       [[ 83, 121, 125],\n",
       "        [ 86, 123, 127],\n",
       "        [ 93, 131, 133],\n",
       "        ...,\n",
       "        [206, 209, 207],\n",
       "        [205, 208, 206],\n",
       "        [205, 208, 206]],\n",
       "\n",
       "       [[ 85, 123, 127],\n",
       "        [ 86, 124, 129],\n",
       "        [ 95, 132, 136],\n",
       "        ...,\n",
       "        [206, 209, 207],\n",
       "        [205, 208, 206],\n",
       "        [204, 207, 205]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[137, 143, 150],\n",
       "        [145, 151, 156],\n",
       "        [146, 152, 159],\n",
       "        ...,\n",
       "        [175, 180, 183],\n",
       "        [175, 180, 181],\n",
       "        [176, 181, 182]],\n",
       "\n",
       "       [[132, 139, 148],\n",
       "        [139, 145, 152],\n",
       "        [144, 150, 157],\n",
       "        ...,\n",
       "        [173, 178, 181],\n",
       "        [176, 181, 184],\n",
       "        [175, 180, 181]],\n",
       "\n",
       "       [[128, 135, 144],\n",
       "        [135, 141, 148],\n",
       "        [140, 146, 153],\n",
       "        ...,\n",
       "        [175, 180, 183],\n",
       "        [175, 180, 183],\n",
       "        [177, 182, 185]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.rectangle(img , (x0 , y0) , (x0 + w0 , y0 + h0) , (0 , 0 , 255) , 3)\n",
    "label = \"{},{}\".format(demography[0][\"dominant_gender\"], demography[0][\"age\"])\n",
    "cv2.putText(img, label, (x0, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# img_face = cv2.rectangle(img , (x1 , y1) , (x1 + w1 , y1 + h1) , (0 , 0 , 255) , 3)\n",
    "# label1 = \"{},{}\".format(demography[1][\"dominant_gender\"], demography[1][\"age\"])\n",
    "# cv2.putText(img_face, label1, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "# img_face = cv2.rectangle(img , (x2 , y2) , (x2 + w2 , y2 + h2) , (0 , 0 , 255) , 3)\n",
    "# img_face = cv2.rectangle(img , (x3 , y3) , (x3 + w3 , y3 + h3) , (0 , 0 , 255) , 3)\n",
    "# img_face = cv2.rectangle(img , (x4 , y4) , (x4 + w4 , y4 + h4) , (0 , 0 , 255) , 3)\n",
    "# img_face = cv2.rectangle(img , (x5 , y5) , (x5 + w5 , y5 + h5) , (0 , 0 , 255) , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"image\", img)\n",
    "cv2.waitKey(0) \n",
    "  \n",
    "# closing all open windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_analyzer(frame):\n",
    "    faces = DeepFace.analyze(img_path = frame, detector_backend = \"ssd\")\n",
    "    image = frame\n",
    "    for face in faces:\n",
    "        x , y , w , h  = face[\"region\"].values()\n",
    "        cv2.rectangle(image , (x , y) , (x + w , y + h) , (0 , 0 , 255) , 3)\n",
    "        label = \"{}-{}-{}\".format(face[\"dominant_gender\"], face[\"age\"], face[\"dominant_emotion\"])\n",
    "        cv2.putText(image, label, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]   \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]   \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]   \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s]   \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]   \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.87it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s]   \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s]   \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.84it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.72it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing video\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "# ret, frame = cap.read()\n",
    "# frame_height, frame_width, _ = frame.shape\n",
    "# out = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "print(\"Processing Video...\")\n",
    "while cap.isOpened():\n",
    "  ret, frame = cap.read()\n",
    "  result = face_analyzer(frame)\n",
    "  cv2.imshow('Deteksi Wajah', result)\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Done processing video\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AudienceAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
